version: "3.8"

services:
  django:
    build:
      context: .
    image: django:latest
    container_name: logistics_django
    volumes:
      - .:/app
    ports:
      - 8000:8000
    networks:
      - elasticnet
    depends_on:
      - elasticsearch
      - redis
      - llama
    # entrypoint: ["./wait-for-it.sh"]
    environment:
      - ELASTICSEARCH_HOST=elasticsearch
      - ELASTICSEARCH_PORT=9200
      - DJANGO_SETTINGS_MODULE=logistics.settings
      - ELASTICSEARCH_URL=http://elasticsearch:9200

    command: bash -c "python /app/manage.py makemigrations  &&  python /app/manage.py migrate &&  uvicorn logistics.asgi:application --host 0.0.0.0 --port 8000"

  elasticsearch:
    container_name: logistics_elasticsearch
    image: docker.elastic.co/elasticsearch/elasticsearch:8.13.0
    volumes:
      - esdata01:/usr/share/elasticsearch/data
    ports:
      - 9200:9200
    environment:
      - discovery.type=single-node
      - xpack.security.enabled=false
      - "ES_JAVA_OPTS=-Xms256m -Xmx256m"
    networks:
      - elasticnet

  redis:
    container_name: logistics_redis
    image: redis:latest
    ports:
      - "6379:6379"
    networks:
      - elasticnet
    volumes:
      - redis_data:/data

  celery:
    build: .
    container_name: logistics_celery
    command: celery -A logistics worker -l info
    volumes:
      - .:/app
    environment:
      - DJANGO_SETTINGS_MODULE=logistics.settings
      - ELASTICSEARCH_URL=http://elasticsearch:9200
    depends_on:
      - django
      - elasticsearch
      - redis
      - llama
    networks:
      - elasticnet

  llama:
    build:
      context: .
      dockerfile: dockerfile.ollama
    container_name: llama2
    networks:
      - elasticnet
    ports:
      - "11434:11434"
    environment:
      - OLLAMA_HOST=0.0.0.0
    command: >
      sh -c "ollama serve &
             sleep 5 &&
             if ollama list | grep -q 'llama2'; then echo 'Modelo llama2 ya disponible'; else echo 'Descargando modelo llama2...' && ollama pull llama2; fi &&
             tail -f /dev/null"

  # local-llm:
  #   image: ghcr.io/huggingface/text-generation-inference:1.4
  #   container_name: local_llm
  #   ports:
  #     - 8080:80
  #   volumes:
  #     - ./models:/data/models
  #   environment:
  #     - MODEL_ID=meta-llama/Llama-2-7b-chat-hf
  #     - QUANTIZE=bitsandbytes
  #   networks:
  #     - elasticnet

  # frontend:
  #   build:
  #     context: ../gadgetgalaxy
  #   image: gadgetgalaxy:latest
  #   container_name: gadgetgalaxy_frontend
  #   volumes:
  #     - ../gadgetgalaxy:/app
  #   ports:
  #     - 3000:3000
  #   networks:
  #     - elasticnet
  #   environment:
  #     - REACT_APP_API_URL=http://localhost:8000
  #   command: ["npm", "run", "dev"]

volumes:
  esdata01:
  postgres_data:
  elasticsearch_data:
  redis_data:

networks:
  elasticnet:
    driver: bridge
